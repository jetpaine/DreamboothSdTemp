import os
import time

import numpy as np
import torch
import torchvision
from PIL import Image
from omegaconf import OmegaConf
from pytorch_lightning.callbacks import Callback
from pytorch_lightning.utilities import rank_zero_info
from pytorch_lightning.utilities.distributed import rank_zero_only
from safetensors.torch import save_file


class SetupCallback(Callback):
    def __init__(self, resume, now, logdir, ckptdir, cfgdir, config, lightning_config):
        super().__init__()
        self.resume = resume
        self.now = now
        self.logdir = logdir
        self.ckptdir = ckptdir
        self.cfgdir = cfgdir
        self.config = config
        self.lightning_config = lightning_config
        # Default to False if not specified in config
        self.use_safetensors = self.config.get("use_safetensors", False)

    def on_keyboard_interrupt(self, trainer, pl_module):
        if trainer.global_rank == 0 and trainer.global_step > 0:
            if self.use_safetensors:
                print(f"Keyboard Interrupt: Saving checkpoint 'last.safetensors' at step {trainer.global_step}...")
                save_path = os.path.join(self.ckptdir, "last.safetensors")
            else:
                print(f"Keyboard Interrupt: Saving checkpoint 'last.ckpt' at step {trainer.global_step}...")
                save_path = os.path.join(self.ckptdir, "last.ckpt")
            
            self.save_checkpoint(trainer, save_path, self.use_safetensors)

    def save_checkpoint(self, trainer, save_path, save_as_safetensors=False):
        """Save model checkpoint in either safetensors or ckpt format"""
        if save_as_safetensors:
            state_dict = trainer.model.state_dict()
            save_file(state_dict, save_path)
        else:
            trainer.save_checkpoint(save_path)

    def on_fit_start(self, trainer, pl_module):
        if trainer.global_rank == 0:
            print("Project config")
            print(OmegaConf.to_yaml(self.config))
            print("Lightning config")
            print(OmegaConf.to_yaml(self.lightning_config))

            OmegaConf.save(self.config, os.path.join(self.cfgdir, "{}-project.yaml".format(self.now)))
            OmegaConf.save(OmegaConf.create({"lightning": self.lightning_config}), os.path.join(self.cfgdir, "{}-lightning.yaml".format(self.now)))
            
            # Log checkpoint format being used
            if self.use_safetensors:
                print("Using SafeTensors format for checkpoints")
            else:
                print("Using ckpt format for checkpoints")
                
    def on_save_checkpoint(self, trainer, pl_module, checkpoint):
        """Hook to override default checkpoint saving behavior"""
        if trainer.global_rank == 0 and self.use_safetensors:
            # We'll handle safetensors saving in our custom save method
            # This hook just prevents the default save for safetensors case
            return True


class ImageLogger(Callback):
    def __init__(self, batch_frequency, max_images, clamp=True, increase_log_steps=True,
                 rescale=True, disabled=False, log_on_batch_idx=False, log_first_step=False,
                 log_images_kwargs=None):
        super().__init__()
        self.rescale = rescale
        self.batch_freq = batch_frequency
        self.max_images = max_images
        # self.logger_log_images = {
        #    pl.loggers.TestTubeLogger: self._testtube,
        # }
        self.log_steps = [2 ** n for n in range(int(np.log2(self.batch_freq)) + 1)]
        if not increase_log_steps:
            self.log_steps = [self.batch_freq]
        self.clamp = clamp
        self.disabled = disabled
        self.log_on_batch_idx = log_on_batch_idx
        self.log_images_kwargs = log_images_kwargs if log_images_kwargs else {}
        self.log_first_step = log_first_step

    # @rank_zero_only
    # def _testtube(self, pl_module, images, batch_idx, split):
    #    for k in images:
    #        grid = torchvision.utils.make_grid(images[k])
    #        grid = (grid + 1.0) / 2.0  # -1,1 -> 0,1; c,h,w
    #
    #        tag = f"{split}/{k}"
    #        pl_module.logger.experiment.add_image(
    #            tag, grid,
    #            global_step=pl_module.global_step)

    @rank_zero_only
    def log_local(self, save_dir, split, images,
                  global_step, current_epoch, batch_idx):
        root = os.path.join(save_dir, "images", split)
        for k in images:
            grid = torchvision.utils.make_grid(images[k], nrow=4)
            if self.rescale:
                grid = (grid + 1.0) / 2.0  # -1,1 -> 0,1; c,h,w
            grid = grid.transpose(0, 1).transpose(1, 2).squeeze(-1)
            grid = grid.numpy()
            grid = (grid * 255).astype(np.uint8)
            filename = "{}_globalstep-{:05}_epoch-{:01}_batch-{:04}.jpg".format(
                k,
                global_step,
                current_epoch,
                batch_idx)
            path = os.path.join(root, filename)
            os.makedirs(os.path.split(path)[0], exist_ok=True)
            Image.fromarray(grid).save(path)

    def log_img(self, pl_module, batch, batch_idx, split="train"):
        check_idx = batch_idx if self.log_on_batch_idx else pl_module.global_step
        if (self.check_frequency(check_idx) and  # batch_idx % self.batch_freq == 0
                hasattr(pl_module, "log_images") and
                callable(pl_module.log_images) and
                self.max_images > 0):
            logger = type(pl_module.logger)

            is_train = pl_module.training
            if is_train:
                pl_module.eval()

            with torch.no_grad():
                images = pl_module.log_images(batch, split=split, **self.log_images_kwargs)

            for k in images:
                N = min(images[k].shape[0], self.max_images)
                images[k] = images[k][:N]
                if isinstance(images[k], torch.Tensor):
                    images[k] = images[k].detach().cpu()
                    if self.clamp:
                        images[k] = torch.clamp(images[k], -1., 1.)

            self.log_local(pl_module.logger.save_dir, split, images,
                           pl_module.global_step, pl_module.current_epoch, batch_idx)

            # print("TODO: LOG IMAGE")
            # logger_log_images = self.logger_log_images.get(logger, lambda *args, **kwargs: None)
            # logger_log_images(pl_module, images, pl_module.global_step, split)

            if is_train:
                pl_module.train()

    def check_frequency(self, check_idx):
        if ((check_idx % self.batch_freq) == 0 or (check_idx in self.log_steps)) and (
                check_idx > 0 or self.log_first_step):
            try:
                self.log_steps.pop(0)
            except IndexError as e:
                print(e)
                pass
            return True
        return False

    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):
        if not self.disabled and (pl_module.global_step > 0 or self.log_first_step):
            self.log_img(pl_module, batch, batch_idx, split="train")

    def on_validation_end(self, trainer: "pl.Trainer", pl_module: "pl.LightningModule", *args) -> None:
        pass
        # if not self.disabled and pl_module.global_step > 0:
        # self.log_img(pl_module, batch, batch_idx, split="val")
        # if hasattr(pl_module, 'calibrate_grad_norm'):
        # if (pl_module.calibrate_grad_norm and batch_idx % 25 == 0) and batch_idx > 0:
        # self.log_gradients(trainer, pl_module, batch_idx=batch_idx)


class CUDACallback(Callback):
    # see https://github.com/SeanNaren/minGPT/blob/master/mingpt/callback.py
    def on_train_epoch_start(self, trainer, pl_module):
        # Reset the memory use counter
        # torch.cuda.reset_peak_memory_stats(trainer.root_gpu)
        # torch.cuda.synchronize(trainer.root_gpu)
        torch.cuda.reset_peak_memory_stats(trainer.strategy.root_device.index)
        torch.cuda.synchronize(trainer.strategy.root_device.index)
        self.start_time = time.time()

    def on_train_epoch_end(self, trainer, pl_module):
        # torch.cuda.synchronize(trainer.root_gpu)
        # max_memory = torch.cuda.max_memory_allocated(trainer.root_gpu) / 2 ** 20
        torch.cuda.synchronize(trainer.strategy.root_device.index)
        max_memory = torch.cuda.max_memory_allocated(trainer.strategy.root_device.index) / 2 ** 20
        epoch_time = time.time() - self.start_time

        try:
            max_memory = trainer.training_type_plugin.reduce(max_memory)
            epoch_time = trainer.training_type_plugin.reduce(epoch_time)

            rank_zero_info(f"Average Epoch time: {epoch_time:.2f} seconds")
            rank_zero_info(f"Average Peak memory {max_memory:.2f}MiB")
        except AttributeError:
            pass


class ModeSwapCallback(Callback):

    def __init__(self, swap_step=2000):
        super().__init__()
        self.is_frozen = False
        self.swap_step = swap_step

    def on_train_epoch_start(self, trainer, pl_module):
        if trainer.global_step < self.swap_step and not self.is_frozen:
            self.is_frozen = True
            trainer.optimizers = [pl_module.configure_opt_embedding()]

        if trainer.global_step > self.swap_step and self.is_frozen:
            self.is_frozen = False
            trainer.optimizers = [pl_module.configure_opt_model()]